\begin{abstract}

To acquire advanced vocabulary (CEFR B2+), English learners often use mnemonic devices, memorable associations linking a new concept to learned concepts to improve memory and recall. We consolidated characteristics of good mnemonics and propose the usage and creation of \textbf{linguistically grounded mnemonics}, which better link to the target vocabulary, improving long-term retention and linguistic knowledge. We investigated whether Large Language Models can consistently help write such effective mnemonics, with two different settings: in-context learning, and reasoning distillation. Concretely, we first measured different prompting strategies with frontier models and generated \links, a synthetic dataset of 2000 triplets of \textit{reasoning trace, mnemonic, and example sentence} for 2000 vocabulary useful for TOEFL iBT \footnote{Internet-based Test of English as a Foreign Language}, IELTS Academic \footnote{International English Language Testing System}, and SAT\footnote{Scholastic Aptitude Test}. Second, using a subset of \links, we distilled linguistic reasoning from the \textit{teacher model}, \teachermodel, to the \textit{student model}, \studentmodel\footnote{\url{https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d}} , with online reinforcement learning. The trained, quantized model can be served with local applications such as OpenWebUI (interface) and Ollama (command-line).

Preliminary evaluation shows

The project examplifies that carefully designed NLP systems can generate resources for language learning, for both classroom settings and self-study.\footnote{\url{https://github.com/chiffonng/mnemonic-gen}}

\end{abstract}
