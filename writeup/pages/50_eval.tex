\section{Evaluation} \label{sec:evaluation}
We evaluated the performance of \linksys in two main ways: \numlist{1} qualitative grading with LLM-as-a-judge and \numlist{2} pairwise preference using double-blind annotations. The first method involved using another LLM, \judgemodel, to evaluate the quality of mnemonics generated by our model, while the second involved annotators comparing mnemonics generated by the base model, \studentmodel, and our model, \linksys.

\input{figures/significance.tex}

\subsection{Qualitative grading with LLM judge} \label{sec:qualitative-llm-judge}

We designed a structured evaluation protocol using \judgemodel as a judge to assess mnemonic quality. The LLM judge evaluated 200 pairs of mnemonics from our test set, with each pair generated by the base model (\studentmodel) and our model \linksys.

We asked the judge to score each mnemonic independently on four metrics from our VAM model (\Cref{fig:good-bad-mnemonics}):
\numlist{1} whether the vocabulary is used correctly in the mnemonic,
\numlist{2} strength of association between the vocabulary and the mnemonic,
\numlist{3} how clear and easy to understand the mnemonic is,
\numlist{4} how memorable the mnemonic is, considering factors like concreteness, imageability, and distinctiveness. The last metric is \numlist{5} whether the mnemonic is linguistically grounded, meaning it incorporates linguistic features such as phonetics, morphology, or etymology.

Each criterion was evaluated on a binary scale (correct usage, linguistic grounding) or a 5-point Likert scale (association, clarity, memorability). The judge was instructed to provide six-field output: the score for each metric, and a brief reasoning for those scores.

We also calculated whether the difference in ratings was statistically significant using \numlist{1} a Wilconoxon signed-rank test for Likert ratings with paired samples and \numlist{2} a McNemar's test for paired boolean ratings. We set the significance level at 0.05.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figures/boolean_comparison.pdf}
  \caption{LLM-as-a-judge evaluation for boolean metrics (true/false): correct usage of vocabulary in mnemonic and linguistic grounding of mnemonic. \linksys shows improvement in both metrics, with notable gains in linguistic grounding (68\% vs. 82\%).}
  \label{fig:llm-judge-boolean}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figures/likert_distribution.pdf}
  \caption{LLM-as-a-judge evaluation for 5-point Likert scale metrics. \linksys shows improvements across all three metrics, with the most significant gains in memorability (mean of 2.6 vs. 2.9)}
  \label{fig:llm-judge-likert}
\end{figure}

We reported the distribution of \judgemodel's ratings in \Cref{fig:llm-judge-boolean,fig:llm-judge-likert}, and summarized the difference in performance between the base model (\studentmodel) and our model in \Cref{tab:significance-llm-judge}. The results indicate that \linksys outperforms the base model across all evaluation metrics. Four of the five metrics showed statistically significant improvements ($p < 0.05$). The most substantial improvement was observed in memorability, with a mean difference of $+0.3$ points and a medium effect size (Cohen's $d = 0.566$). Clarity and semantic association also showed significant improvements with medium effect sizes.

While correct vocabulary usage showed a positive trend, this difference was not statistically significant ($p = 0.077$). This suggests that both models were already competent at using vocabulary correctly, with less room for improvement in this area.

\subsection{Pairwise preference using blind annotations} \label{sec:pairwise-preference}

To directly compare the quality of mnemonics, we conducted a blind annotation study. We randomly selected 100 mnemonics from our test set, with 50 each generated by the base model (\studentmodel) and our model (\linksys). Two annotators (the author and an LLM) were asked to evaluate the mnemonics in pairs, choosing the better mnemonic for each pair while being blinded to the model used and the order of mnemonic presented in a pair\footnote{We acknowledge both using the author and using an LLM for evaluation as limitations in \Cref{sec:limitations}.}. This approach allowed us to understand the relative performance of each model. The annotators were instructed to consider the same criteria as in the LLM-as-a-judge evaluation, and indicated which mnemonic they preferred and why. More details in \Cref{app:annotation}.

\Cref{fig:pairwise-preference} reveals a strong preference for mnemonics generated by \linksys over those from the base model (64\% vs. 36\%). Annotators particularly valued mnemonics that incorporated multiple linguistic features and provided clear, concrete associations to abstract concepts.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figures/model_comparison.pdf}
  \caption{Pairwise preference using double-blind annotation. Y-axis shows the percentage of preference for each mnemonic generation method.}
  \label{fig:pairwise-preference}
\end{figure}
