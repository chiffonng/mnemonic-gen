\section{Introduction} \label{sec:intro}

%%%% Figure Highlight the difference between keyword "mnemonic" and "linguistically grounded mnemonics" with an example
%%% (e.g., \textbf{preposterous} can be broken down as pre- (before) + poster (after) + ous. Anything that comes both before and after is preposterous)

Vocabulary acquisition challenges many English language learners, particularly at upper intermediate to advanced levels where abstract and academic vocabulary predominates. For effective vocabulary learning, deeper linguistic engagement that connects new vocabulary to existing knowledge is essential. Large Language Models (LLMs) have demonstrated capabilities as knowledge bases and creative text generators, suggesting their potential for automated language learning assistance.

Previous work explored automated mnemonic generation through computational methods using the \textbf{keyword method} \citep{atkinsonApplicationMnemonicKeyword1975}. \citet{savvaTransPhonerAutomatedMnemonic2014} and \citet{OzbalAUTOMATION2014} generated keywords of phonetic and orthographic similarities in the native language for foreign language vocabulary, across multiple languages. \citet{LeeSMARTPHONE2023} extended this work and utilized LLMs to produce phonetically similar keywords and visual cues and \citet{LeeEXPLORING2024} prompted LLMs to generate multiple mnemonic candidates and evaluate them based on imageability and coherence. Most recently, \citet{BalepurSMART2024} fine-tuned LLaMA-2-70B on 800 crowd-sourced mnemonics and aligned outputs with learner preferences and learning outcomes.

These approaches, however, have primarily focused on the keyword method while neglecting other rich linguistic knowledge embedded in LLMs. Additionally, they typically deliver generated mnemonics passively to learners or aggregated learner preferences, which may not align with individual learning styles. For instance, \textsc{Smart} \citep{BalepurSMART2024} was integrated into a Spaced Repetition System (SRS) that tested learners' knowledge, it did not actively engage learners in the mnemonic creation process. \textsc{Smart} also aggregated learner preferences and outcomes, potentially missing alignment with individual learning styles. Research shows that self-created mnemonics lead to more effective and longer-lasting vocabulary retention \citep{madanExploringWordMemorability2021}.

Our research investigates whether LLMs can generate linguistically grounded mnemonics that leverage multiple linguistic features beyond simple keyword associations. Our key questions include: \numlist{1} Can LLMs, particularly reasoning ones, leverage various linguistic features for mnemonic generation without further training? \numlist{2} If so, can we distill these capabilities to smaller, locally deployable models? \numlist{3} Are LLM-generated mnemonics comparably helpful to human-generated mnemonics in terms of memorability and retention?

In this paper, we demonstrate that LLMs can generate linguistically grounded mnemonics through reasoning and creative writing (\Cref{sec:icl-performance}). We present \links, a synthetic dataset of 2000 triplets of \textit{reasoning trace, mnemonic, and example sentence} for vocabulary terms, suitable for integration with SRS or language learning applications. \numlist{3} We distill the reasoning capabilities of a teacher model (\teachermodel) into a smaller student model (\studentmodel) (\Cref{sec:distillation}). The resulting model, \linksys, can be deployed locally, enabling users to generate mnemonics without relying on external APIs.
