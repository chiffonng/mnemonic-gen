
\begin{titlepage}
\centering
{\scshape\LARGE Minerva University \par}
\vspace{1cm}
\begin{center}
  \includegraphics[width=0.4\linewidth]{preamble/minerva_logo.pdf}
\end{center}
{\scshape\Large Capstone: Class of 2025 \par}
\vspace{1.5cm}
{\huge\bfseries LINKS: Generate linguistically grounded mnemonic devices for English vocabulary learning with reasoning, multilingual LLMs \par}
\vspace{2cm}
{\scshape\large Tra My (Chiffon) Nguyen \par}

\vfill
submitted in partial fulfillment of the requirements for the degree of \\ Bachelor of Science in Computational Sciences \par
\vspace{2cm}
{\large Capstone Committee \par}
Dr. Patrick Watson \\
Dr. Philip Sterne \\
\vspace{2cm}
{\large \today\par}
\end{titlepage}

\onecolumn
\section*{Executive Summary}
%% TODO: Explain the project to a non-technical audience.
% - What is the problem?
% - What is the solution?
% - What is the impact?
% - What is the next step?

Tags: computational linguistics, natural language processing, large language model, language education, english as a foreign language, vocabulary acquisition, synthetic data generation.

Note:

\subsection*{AI Statement}
\subsection*{Important Notes}

Due to limited compute, some experiments conducted are small-scale and need more data for robust validation and conclusion. However, the codebase is reproducible and scalable when there is more compute. All links, including this paper source .tex, is included on \hyperlink{https://github.com/chiffonng/mnemonic-gen}{Github}.

This project went through multiple technical iterations behind the scene, from supervised finetuning (Nov 2024) to group releative policy optimization (Feb 2025) (for details refer to \Cref{app:previous-iterations}). The main paper only discussed the final iteration, which is the most promising one. The other iterations are not included in the paper but are available in the codebase.
\clearpage

\tableofcontents
