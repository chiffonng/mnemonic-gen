\begin{titlepage}
\centering
{\scshape\LARGE Minerva University \par}
\vspace{1cm}
\begin{center}
  \includegraphics[width=0.4\linewidth]{minerva/minerva_logo.pdf}
\end{center}
{\scshape\Large Capstone: Class of 2025 \par}
\vspace{1.5cm}
{\huge\bfseries LINKS: Generate linguistically grounded mnemonic devices for English vocabulary learning with reasoning, multilingual LLMs \par}
\vspace{2cm}
{\scshape\large Tra My (Chiffon) Nguyen \par}

\vfill
submitted in partial fulfillment of the requirements for the degree of \\ Bachelor of Science in Computational Sciences \par
\vspace{2cm}
{\large Capstone Committee \par}
Dr. Patrick Watson \\
Dr. Philip Sterne \\
\vspace{2cm}
{\large \today\par}
\end{titlepage}

\onecolumn
\section*{Executive Summary} \label{sec:exec-summary}

Learning new vocabulary is a universal challenge faced by language learners worldwide. Traditional approaches often rely on rote memorization, which can be tedious and ineffective for many learners. Our research tackles this problem by developing an innovative approach to vocabulary learning through "linguistically grounded mnemonics", memory aids that leverage a word's linguistic features such as etymology, sound patterns, or word structure to create meaningful connections that make words easier to remember.

I've created a specialized AI system called \linksys that generates mnemonics for English vocabulary words. Building on the keyword method that associates new, foreign words with familiar words, our system goes a step further by creating mnemonics that incorporates linguistic rich linguistic characteristics of words, helping learners understand not just what words mean but why they mean what they do. For example, instead of simply memorizing that "adumbrate" means "to outline or foreshadow," our system explains that it comes from Latin roots "ad-" (to, on) + "umbra" (shade), creating a mnemonic that connects the word's meaning to its origin. The system generates mnemonics that are not only linguistically rich but also memorable.

My approach combines advanced AI techniques to distill linguistic reasoning into a compact, accessible model that outperforms larger systems while requiring fewer computational resources. This makes it practical for deployment in educational settings, even with limited technology budgets. Through rigorous testing, we found that learners consistently preferred our system's mnemonics over those generated by standard AI models. This research represents a step toward more effective, personalized vocabulary learning tools that can adapt to different learning styles and preferences, potentially transforming how we approach language education.

Tags: computational linguistics, natural language processing, large language model, language education, english as a foreign language, vocabulary acquisition, synthetic data generation.

\subsection*{AI Statement} \label{sec:ai-statement}

The main idea of this project is to use large language models (LLMs) to generate mnemonic devices for English vocabulary learning. Such AI usage is documented in the main paper.

I extensively used Claude 3.7 Sonnet connected with my codebase to \numlist{1} generate working Python code for major features and plots, \numlist{2} debug my code and \numlist{3} iteratively improve my codebase with best practices including refactoring, modularization, and documentation. I also used Claude to aid producing this paper by \numlist{4} generating LaTex figures and tables, such as \Cref{fig:good-bad-mnemonics} and \Cref{tab:significance-llm-judge}, \numlist{5} explaining new methods used, especially GRPO (\Cref{app:grpo}), and \numlist{6} rewriting some sections of the paper to improve clarity and conciseness, particularly Abstract and \Cref{sec:discussion}.

There were several instances where AI failed to help, mostly due to \numlist{1} the usage of AI-related knowledge and packages that are recently released or updated, such as \verb|trl|'s \verb|GRPOTrainer| class, or and \numlist{2} low-frequency knowledge, such as some TeX formatting or lower-level software or hardware issues. An example: I failed to send API requests to \teachermodel from \verb|curator| but not from \verb|openai| library, due to SSL certificate issue. The root cause was a conflict between different Python versions installed globally on my laptop, one from \verb|homebrew| and one from Python distribution.

\subsection*{Important Notes} \label{sec:important-notes}

Due to limited compute, some experiments conducted are small-scale and need more data for robust validation and conclusion. However, the codebase is reproducible and scalable when there is more compute. All links, including this paper source .tex, is included on \hyperlink{https://github.com/chiffonng/mnemonic-gen}{Github}.

This project went through multiple technical iterations behind the scene, from supervised finetuning (Nov 2024) to group releative policy optimization (Feb 2025) (for details refer to \Cref{app:previous-iterations}). The main paper only discussed the final iteration, which is the most promising one. The other iterations are not included in the paper but their implementation is available in the codebase in forms of Jupyter Notebooks and old pull requests.

\clearpage

\tableofcontents
