
\appsection{Technical preliminaries} \label{app:technicality}

\appsubsection{In-context learning} \label{sec:icl-info}

\appsubsubsection{Chain-of-Thought (CoT) prompting} CoT \citep{weiChainofThoughtPromptingElicits2022} is a prompting technique that encourages LLMs to generate intermediate reasoning steps before arriving at a final answer. This approach has been shown to improve performance on complex tasks by guiding the model through a structured thought process.

\appsubsection{Neural Language Models and Transformer Architecture} \label{app:llm-transformer}

Neural language models are probabilistic frameworks that assign probabilities to sequences of words or subword units, known as tokens. A token is the smallest unit of text that the model processes, which can be as granular as individual characters, subwords, or entire words, depending on the tokenization strategy employed.

Given a sequence of tokens \( \mathbf{x} = (x_1, x_2, \ldots, x_T) \), a language model estimates the joint probability \( P(\mathbf{x}) \) by factorizing it into conditional probabilities:

\begin{equation}
P(\mathbf{x}) = \prod_{t=1}^T P(x_t \mid x_1, x_2, \ldots, x_{t-1})
\end{equation}

At each time step \( t \), the model predicts the next token \( x_t \) based on preceding sequence \( (x_1, x_2, \ldots, x_{t-1}) \). This autoregressive approach enables the generation of coherent text by sequentially predicting subsequent tokens.

The Transformer architecture underpins many state-of-the-art language models due to its efficiency and capability to model long-range dependencies. It utilizes self-attention mechanisms to weigh the relevance of each token in a sequence relative to others, regardless of their positions. The architecture comprises stacked layers, each including multi-head self-attention and position-wise feed-forward networks, facilitating parallelization and effective learning of complex patterns in data.

\subsubsection{Tokenizer} \label{app:tokenizer}

A tokenizer is a preprocessing tool that converts raw text into tokens, aligning the text with the LM's vocabulary. Tokenizers can employ various strategies, such as word-based, character-based, or subword-based tokenization, each with distinct advantages and use cases.

Byte Pair Encoding (BPE) is a subword tokenization algorithm that operates on the byte representation of text, enabling consistent handling of various scripts and special characters. It iteratively merges the most frequent pairs of adjacent bytes to form subword units, constructing a vocabulary that efficiently represents the training corpus. This method allows the tokenizer to decompose rare words into meaningful subword components, enhancing the model's capacity to process diverse and unseen terms.

For instance, the word "preposterous" might be tokenized into subwords like "pre", "poster", and "ous," facilitating the model's understanding and generation of these subwords in novel contexts. This subword granularity enables the model to generalize across morphologically complex words and out-of-vocabulary words, enhancing its robustness and vocabulary coverage. However, not all subwords are valid morphemes, which can limit the model's ability to capture morphological structure accurately. For instance, \texttt{tiktoken} (OpenAI's tokenizer)\footnote{\href{https://platform.openai.com/tokenizer}{https://platform.openai.com/tokenizer}} recognizes "ephemeral" as a single subword rather than three morphemes ("ept", "hemera", "-al"), because the affixes are not explicitly segmented, and 'epheremal' is a rare word so BPE better learns it as a single token.

\appsubsection{Family of Fine-Tuning Methods} \label{app:finetuning}
Fine-tuning is the process of adapting a pre-trained model to a specific task T or domain D by updating its parameters on a target dataset \(\mathcal{D}\). This process is crucial for leveraging pre-trained models' knowledge and enhancing their performance on downstream tasks.

There are several approaches to fine-tuning, which can be categorized by: 1. the availability of labeled data (supervised vs unsupervised fine-tuning), 2. the extent of parameter updates (full-parameter vs parameter-efficient fine-tuning), and 3. task. We focus on supervised fine-tuning, which involves minimizing a task-specific loss function over a labeled dataset.

\appsubsubsection{Supervised Fine-Tuning (SFT)}\label{app:sft}

SFT involves adapting a pre-trained model to a target task by minimizing a task-specific loss function over a labeled dataset. For a dataset \( \mathcal{D} = \{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^N \), where \( \mathbf{x}^{(i)} \) is the input and \( \mathbf{y}^{(i)} \) is the target output, the objective is to minimize:

\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{i=1}^N \ell(f(\mathbf{x}^{(i)}; \theta), \mathbf{y}^{(i)})
\end{equation}

where \( f(\mathbf{x}; \theta) \) represents the model's output with parameters \( \theta \), and \( \ell \) is the loss function, typically cross-entropy loss.

\appsubsubsection{Instruction tuning} \label{app:instruction-tuning-it}

Instruction-tuning is a specialized form of SFT \Cref{app:sft} where models are trained on datasets comprising instruction-response pairs. This approach enables models to generalize across various tasks described by natural language instructions, enhancing their ability to follow diverse prompts. Formally, an instruction-tuning dataset consists of pairs \( \{(\mathbf{I}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^N \) or triplets \( \{(\mathbf{I}^{(i)}, \mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^N \), where \( \mathbf{I}^{(i)} \) denotes the instruction, \( \mathbf{x}^{(i)} \) is the optional input, and \( \mathbf{y}^{(i)} \) is the desired output. The training objective is to minimize the loss:

\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{i=1}^N \ell(f(\mathbf{I}^{(i)}, \mathbf{x}^{(i)}; \theta), \mathbf{y}^{(i)})
\end{equation}

where \( f \) represents the model parameterized by \( \theta \), and \( \ell \) is the loss function measuring the discrepancy between the model's prediction and the target output.

\appsubsubsection{Parameter-Efficient Fine-Tuning} \label{app:peft}
Full-parameter fine-tuning updates \textit{all} parameters of a pre-trained model on the target dataset, which can be computationally expensive and memory-intensive for large models. Parameter-efficient fine-tuning (PEFT) methods adjust only a subset of the parameters, reducing computational and storage requirements while maintaining performance \citep{XuPARAMETEREFFICIENT2023}.

The most common PEFT method is Low-Rank Adaptation (LoRA), and its variants. They are used in the training process as a wrapper around the model's weights, allowing for efficient updates without modifying the entire model. This approach is particularly useful for large models, where full fine-tuning may be impractical due to resource constraints.

\paragraph{Low-Rank Adaptation (LoRA)} decomposes the weight updates into low-rank matrices, reducing the number of trainable parameters \citep{huLoRALowRankAdaptation2021}. Specifically, for a weight matrix \( W \in \mathbb{R}^{d \times k} \), LoRA introduces two low-rank matrices \( A \in \mathbb{R}^{d \times r} \) and \( B \in \mathbb{R}^{r \times k} \), where \( 0 < r \ll \min(d, k) \). The adapted weight is:

\begin{equation}
W' = W + \alpha \cdot A B
\end{equation}

Here, \( \alpha \) is a scaling factor that controls the contribution of the low-rank adaptation. The rank \( r \) determines the capacity of the adaptation, balancing between expressiveness and efficiency.

LoRA introduces \( 2dr \) trainable parameters (size of \( A \) and \( B \)), which is significantly smaller than the original \( dk \) parameters. This reduction in parameters enables efficient fine-tuning of large models on limited hardware. In practice, LoRA is applied to specific modules of the model, such as attention and feed-forward layers, to balance performance and efficiency.

\paragraph{Rank-Stabilized LoRA (rsLoRA)} modifies the scaling factor in LoRA to improve performance across different ranks. The standard scaling factor \( \gamma_r = \alpha / r \) can slow learning for higher ranks. rsLoRA proposes adjusting the scaling factor to \( \gamma_r = \alpha / \sqrt{r} \), enhancing fine-tuning performance without increasing inference costs.

\subsection{Reinforcement Learning (RL)} \label{app:rl}

Reinforcement Learning (RL) is a framework in which an agent interacts with an environment to learn a policy $\pi_\theta$ that maximizes a long-term reward. At each time step $t$, the agent observes a state, takes an action, and receives a reward $r_t$. The goal is to maximize the expected cumulative reward, given by

\begin{equation}
J(\theta) = \mathbb{E}_{\pi_\theta}\left( \sum_{t=0}^{T} \gamma^t\,r_t\right)
\end{equation}

where $\gamma\in(0,1)$ is a discount factor.

In the next section, we consider an advanced method called Group Relative Policy Optimization (GRPO). GRPO extends PPO by generating multiple responses per prompt, comparing the rewards within each group, and adjusting the policy based on relative advantages. This online RL approach continuously improves by (1) generating completions, (2) scoring them using reward models, and (3) updating the model's policy with both an advantage term and a KL penalty.

\appsubsubsection{Group Relative Policy Optimization} \label{app:grpo}

Group Relative Policy Optimization (GRPO) \citet{DeepSeek-AIDEEPSEEKR12025} is an online reinforcement learning method specifically designed for scenarios where the model generates multiple responses (or completions) for the same prompt. It was introduced to improve the mathematical reasoning capabilities of LLMs, by generating multiple CoT responses for a given problem and then compares results to the ground truth.

Intuitively, GRPO generates multiple responses for a given prompt, scores them using reward models, calculates the relative reward of the group, and then compares each response's score to that relative reward to determine which is better or worse. The model then updates its policy to favor high-reward responses.

\paragraph{Generating completions} For each prompt $q$ in a batch, the model generates a set of $G$ completions:
\begin{equation}
O_q = \{o_1, o_2, \ldots, o_G\}
\end{equation}

Each completion $o_i$ consists of a sequence of tokens:
\begin{equation}
o_i = \{o_{i,1}, o_{i,2}, \ldots, o_{i,|o_i|}\}
\end{equation}

\paragraph{Computing the advantage} For each completion, a reward $r_i$ is computed using predefined reward functions. To enable comparison within groups, the rewards are normalized:
\begin{equation}
\mu_r = \text{mean}(r)
\end{equation}
\begin{equation}
\sigma_r = \text{std}(r)
\end{equation}
\begin{equation}
\hat{A}_{i,t} = \frac{r_i - \mu_r}{\sigma_r}
\end{equation}

where $r = \{r_1, r_2, \ldots, r_G\}$ is the set of rewards for all completions in the group, and $\hat{A}_{i,t}$ is the advantage for token $t$ in completion $i$. This normalization gives the method its name: Group Relative Policy Optimization.

\paragraph{Estimating the KL divergence} To prevent the policy from deviating too far from the reference policy $\pi_{\text{ref}}$, the KL divergence is estimated:
\begin{equation}
\pi_\text{ratio} = \frac{\pi_\theta(o_{i,t} | q, o_{i,<t})}{\pi_{\text{ref}}(o_{i,t} | q, o_{i,<t})}
\end{equation}
\begin{equation}
\pi_\text{inv\_ratio} = \frac{\pi_{\text{ref}}(o_{i,t} | q, o_{i,<t})}{\pi_\theta(o_{i,t} | q, o_{i,<t})}
\end{equation}
\begin{equation}
D_{\text{KL}} = \log\pi_\text{ratio} - 1 + \pi_\text{inv\_ratio}
\end{equation}

\paragraph{Computing the loss} The GRPO objective combines the advantage term with a KL penalty:
\begin{equation}
L_{\text{adv}} = -\frac{1}{G}\sum_{i=1}^{G}\sum_{t=1}^{|o_i|}\pi_\text{ratio}\hat{A}_{i,t}
\end{equation}
\begin{equation}
L_{\text{KL}} = \beta D_{\text{KL}}
\end{equation}
\begin{equation}
L_{\text{GRPO}}(\theta) = L_{\text{adv}} - L_{\text{KL}}
\end{equation}

where $\beta$ is a hyperparameter that controls the weight of the KL penalty. The advantage term encourages the policy to assign higher probability to tokens that lead to better rewards, while the KL term ensures that the policy doesn't deviate too far from the reference policy.

\paragraph{Multiple updates} For multiple $\mu$ updates after each generation, GRPO uses a clipped surrogate objective. First, compute the old policy ratio:
\begin{align}
\pi_{\text{old\_ratio}} &= \frac{\pi_\theta(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})},
\end{align}
then clip it:
\begin{align}
\pi_{\text{clipped}} &= \text{clip}\Bigl(\pi_{\text{old\_ratio}},\, 1-\epsilon,\, 1+\epsilon\Bigr).
\end{align}
The clipped advantage loss is $L_{\text{adv\_clipped}}$
\begin{equation}
-\frac{1}{G}\sum_{i=1}^{G}\sum_{t=1}^{|o_i|}
\min\Bigl(\pi_{\text{old\_ratio}}\hat{A}_{i,t},\, \pi_{\text{clipped}}\hat{A}_{i,t}\Bigr),
\end{equation}
yielding the final objective:
\begin{equation}
L_{\text{GRPO\_clipped}}(\theta) = L_{\text{adv\_clipped}} - L_{\text{KL}}.
\end{equation}

Here, $\epsilon$ (small constant, typically 0.2) controls how much the policy can change in a single update and $\beta$ controls the KL penalty's strength.

In HuggingFace's \texttt{trl} library, GRPO is implemented in the \texttt{GRPOTrainer} class and number of updates $\mu$ is controlled by the \texttt{num\_iterations} parameter. The default value of $\mu = 1$ simplifies the objective to the original GRPO formulation.
