\section{Minerva Appendix: LOs \& HCs} \label{sec:minerva}

\subsection{LOs} \label{sec:los}
\numpara{CS110-codeReadability} The codebase exemplifies best practices in Python programming, adhering strictly to PEP conventions. Each module is documented with detailed Google-style docstrings and descriptive inline comments to ensure that the logic behind functions and classes is transparent to collaborators and future users. By utilizing tools like Ruff for linting and formatting, and mypy for type-checking, the codebase achieves a consistent style and minimizes errors. Additionally, the inclusion of pre-commit hooks ensures that these standards are maintained across all contributions, fostering a robust and maintainable codebase.
\numpara{CS162-communication} The documentation strives to adhere to industry standards, by including an informative README, clear commit messages (using conventional commit or gitmoji), and documented pull request. Each module has a module-level docstring and function-level docstrings, to ensure that the code is understandable to users and collaborators. I used \verb|Ruff| (for linting and black-style code formatting) and \verb|mypy| (for type checking), to enforce consistent and error-free code presentation, to facilitate readability and maintainability for the author and future collaborators (if any).
\numpara{CS156-MLCode} The machine learning pipeline was designed in Python to be both functional and comprehensible. The code integrates data processing, model fine-tuning, and hyperparameter tuning into a seamless pipeline, with each step working and documented. Model evaluation are explicitly defined in appendix \Cref{app:training-details}, ensuring replicability.
\numpara{CS156-MLExplanation} Currently, most details and explanations are defined in section \Cref{app:technicality} and appendix \Cref{app:training-details}. The final paper will provide more high-level diagrams of the machine learning techniques used in fine-tuning the Gemma-2 model. The supporting diagrams will visualize key processes, such as hyperparameter tuning and model evaluation. It will ensure that the methodology and results are accessible to a less specialized audience.
\numpara{CS162-separationofconcerns} The codebase is organized into distinct Python modules, each focused on a specific task such as data processing, mnemonic processing, and model fine-tuning. This separation of concerns aligns with best practices in software design, ensuring that each function is highly cohesive and performs a single well-defined responsibility. By maintaining modularity, the codebase facilitates easier debugging, testing, and future scaling, contributing to its long-term maintainability and effectiveness.

% TODO: Capstone LOs
\subsection{Capstone LOs} \label{sec:capstone-los}
\numpara{navigation} \Cref{app:previous-iterations}
\numpara{outcomeanalysis}
\numpara{curation}
\numpara{qualitydeliverables}

\subsection{HCs} \label{sec:hcs}
\numpara{audience} The project addresses dual audiences: (1) NLP researchers investigating LLMs' linguistic capabilities, for whom we provide detailed technical methodologies and evaluation metrics; and (2) educational technology developers seeking practical approaches to vocabulary learning assistance, for whom we demonstrate application potential and implementation strategies. For example, \Cref{sec:mnemonic-review} introduces key concepts about mnemonic devices and language teaching with sufficient depth for both audiences, while hiding technical details in appendices but providing pointers to those details in each section for interested readers. The paper's structure and content are designed to facilitate knowledge transfer across these domains, ensuring that both audiences can derive value from the research findings. Technical content is balanced with practical implications for vocabulary acquisition and language learning, ensuring relevance to both research and application-oriented readers.

\numpara{organization} The paper follows standard ACL formatting conventions, with a logical progression from theoretical foundations through methodology to empirical results. The structure facilitates efficient information extraction, with each section building upon previous content. Key contributions are identified early (\Cref{sec:intro}) and systematically developed throughout subsequent sections. Technical details that might interrupt argumentative flow are relegated to appendices, maintaining narrative coherence while ensuring methodological transparency for reproduction purposes. This organization aligns with expectations of the computational linguistics community while supporting efficient knowledge transfer.

\numpara{gapanalysis} \Cref{sec:intro} identifies critical limitations in existing mnemonic generation approaches: (1) overreliance on the keyword method, which fails for abstract vocabulary lacking concrete referents, and (2) neglect of the rich linguistic knowledge embedded in LLMs that could enable more diverse mnemonic strategies beyond simple keyword associations. Prior work has also passively delivered mnemonics to learners rather than leveraging individual learning preferences, despite research showing self-created mnemonics enhance retention. These identified gaps motivate our project to elicit linguistic reasoning and creativity from LLMs, enabling them to generate mnemonics that are not only effective but also linguistically grounded. Our final model, \linksys, after deployment, could interact with learners to tailor mnemonic generation based on their preferences, enhancing the learning experience. This approach addresses the limitations of existing methods by providing a more comprehensive and personalized vocabulary learning tool. The project also contributes to the field of educational technology by exploring how LLMs can be harnessed for creating effective resources for language education and self-study, such as creating mnemonic devices.

% TODO
\numpara{hypothesisdevelopment} The research questions in \Cref{sec:intro} establish testable hypotheses regarding LLMs as linguistic knowledge bases for mnemonic generation. We hypothesize that fine-tuning LLMs on linguistically annotated examples will improve mnemonic quality across semantic relevance, diversity, and helpfulness dimensions. This hypothesis is predicated on the theoretical assumption that LLMs encode significant linguistic knowledge during pre-training that can be accessed through targeted fine-tuning. The hypothesis is operationalized through specific evaluation metrics described in \Cref{sec:evaluation}, ensuring empirical testability.

\numpara{scienceoflearning}

\numpara{optimization} ...

\numpara{algorithms} The fine-tuning methodology detailed in \Cref{app:finetuning} incorporates algorithmic innovations in parameter-efficient adaptation. Specifically, we implement QLoRA with rank-stabilized scaling (rsLoRA), modifying the standard scaling factor to improve performance across different ranks. This algorithm reduces memory requirements by quantizing the pre-trained model to 4-bit precision while allowing selective updates to low-rank adaptation matrices. Population-based training explores the hyperparameter space dynamically, pruning underperforming configurations and exploring promising regions to optimize validation performance.

\numpara{heuristics} Our approach employs several problem-solving heuristics to enhance mnemonic generation. The linguistic feature classification system provides a structured framework for identifying and leveraging different linguistic aspects in vocabulary. We use problem decomposition by separating mnemonic creation into linguistic analysis and creative association phases, enabling more systematic knowledge utilization. Visualization techniques in the form of embedding space projections help identify semantic relationships between vocabulary terms and potential mnemonic content. These heuristics guide both the model fine-tuning process and the subsequent evaluation methodology.

\numpara{sampling} The evaluation methodology employs stratified random sampling to ensure representation across linguistic feature categories (\Cref{sec:vocab-selection}) and vocabulary complexity levels. For human evaluation, we randomly selected 50 test examples, stratified by linguistic feature, to obtain less biased assessments of mnemonic quality. This sampling strategy ensures balanced representation of different mnemonic types while maintaining statistical validity. Each example received multiple independent ratings to mitigate individual rater bias, with inter-rater reliability calculated using Cohen's Kappa score to confirm consistency in ratings.

\numpara{dataviz} We employ targeted data visualizations to communicate complex relationships between linguistic features and mnemonic effectiveness. Radar charts display the distribution of linguistic features across different model outputs, while heatmaps visualize correlation patterns between computational metrics and human evaluations. Embedding space projections illustrate semantic relationships between vocabulary terms and their mnemonics, providing intuitive visual confirmation of semantic relevance scores. These visualizations enhance interpretability of results while supporting our conclusions regarding the contribution of different linguistic features to mnemonic quality.

\numpara{significance} Statistical significance testing (\Cref{sec:qualitative-llm-judge}) confirms the reliability of our comparative results between baseline and \linksys. We employ paired statistical tests (Wilcoxon signed-rank and McNemar test) to account for vocabulary-specific variation when comparing model outputs on identical test sets. Effect size calculations quantify the practical significance of improvements, while confidence intervals provide transparency regarding the precision of our estimates. Multiple comparison corrections maintain statistical rigor when evaluating performance across different linguistic feature categories.

\numpara{shapingbehavior} The intended application of our approach shapes vocabulary learning behavior by encouraging deeper engagement with linguistic features. Rather than keyword method, the generated mnemonics prompt learners to recognize morphological, etymological, and semantic patterns, fostering more robust mental representations. By explicitly highlighting these linguistic features, the system promotes analytical processing of vocabulary, which research indicates enhances long-term retention. This behavior-shaping aspect represents a significant advantage over keyword-only approaches that rely on shallow phonetic or orthographic associations.

\numpara{biasmitigation}

\numpara{ethicalconsiderations} Our work addresses ethical considerations in educational technology deployment. We explore linguistic diversity by analyzing and leveraging several linguistic features for mnemonic generation, ensuring that the generated mnemonics are inclusive and accessible to learners from diverse linguistic backgrounds. The model is trained on a diverse dataset of vocabulary words, including those from various languages and etymological origins, to ensure that the mnemonics generated are relevant and culturally sensitive. We also consider the potential for bias in the generated mnemonics by ensuring that the training data is representative of a wide range of vocabulary words.

\numpara{studyreplication} To facilitate replication, we provide comprehensive implementation details in \Cref{app:training-details}, including environment setup, model parameters, and training configurations. The dataset construction process is documented in \Cref{sec:data-gen}, with preprocessing steps explicitly specified. All code and datasets are made publicly available through GitHub and HuggingFace repositories, with standardized formats ensuring compatibility with common ML frameworks. This transparency ensures that other researchers can validate the findings and build upon the methodology.
