\newcounter{para}
\newcommand\numpara{\par\refstepcounter{para}{\thepara}.\space\textbf}

\section{Minerva Appendix: LOs \& HCs} \label{sec:minerva}

\subsection{Computational Sciences LOs} \label{sec:los}

\numpara{CS110-codeReadability} The codebase demonstrates exemplary readability practices throughout its implementation. Each module follows consistent Google-style docstrings with clear descriptions of functionality, parameters, return types, and exceptions. Critical sections like GRPO reward functions in \verb|src/train/reward_functions.py| contain detailed inline comments explaining complex algorithms and design decisions. The project employs tools like Ruff for linting and formatting, and mypy for static type checking, maintaining consistent style across different components. File organization follows a logical structure, separating concerns like data preparation, model training, and evaluation into distinct directories. This disciplined approach to code readability makes the system accessible to potential collaborators and facilitates maintenance and extension.

\numpara{CS156-MLCode} The implementation showcases a comprehensive machine learning pipeline built with modern Python libraries, notably HuggingFace ecosystems, \verb|unsloth|, \verb|vllm|, and more in \Cref{app:training-details}. The core training functionality leverages PyTorch, transformers, \verb|trl|'s GRPOTrainer and custom training utilities to implement GRPO wrapped in LoRA layers. Data management handle preprocessing, stratified sampling, and validation with pandas and HuggingFace datasets. The evaluation modules in \verb|src/eval/| implement diverse metrics for measuring mnemonic quality with statistical significance testing. I wrote performant code, and delegated to specialized libraries for appropriate tasks: curator for API management, trl for GRPO implementation, and unsloth for memory-efficient training. Performance considerations are evident in the quantization approach and batch processing strategies, making the system viable on consumer hardware.

\numpara{CS156-MLExplanation} The paper provides clear and accessible explanations of the technical components through appropriate visualizations and descriptions. The GRPO approach is carefully explained in \Cref{app:grpo} with a mathematical formulation showing reward aggregation across multiple functions. More details and explanations are defined in section \Cref{app:technicality} and appendix \Cref{app:training-details}. \Cref{fig:distillation} visually represents the full distillation pipeline, making the complex workflow understandable to readers without requiring deep technical knowledge. The statistical analysis in Section 5.1 explains LLM-as-a-judge evaluation with appropriate statistical testing using Wilcoxon signed-rank and McNemar tests, reporting effect sizes and significance levels in Table 2. This multi-layered explanation approach balances accessibility for general readers with sufficient technical depth for researchers seeking to build upon this work.

\numpara{CS156-MLFlexibility} The project adapts modern machine learning techniques (GRPO, LoRA) to the novel domain of linguistic mnemonic generation. Rather than applying a standard fine-tuning approach, the system explores multiple training methodologies before settling on GRPO with LoRA as documented in Appendix A. The reward function design shows creative adaptation of reinforcement learning concepts to linguistic quality metrics, blending traditional NLP evaluation with psycholinguistic principles of memorability. When challenged with resource constraints, the implementation pivots to parameter-efficient methods and quantization techniques to maintain performance on available hardware. The meta-knowledge about different model architectures is evident in the comparative analysis between reasoning-specialized models and general-purpose LLMs in Section 3, showing a deep understanding of architectural strengths beyond surface-level benchmarks. This flexibility extends to the evaluation approach, combining automated metrics with human preference studies to provide a well-rounded assessment.

\numpara{CS162-separationofconcerns} The codebase exemplifies clean separation of concerns through its modular architecture. Each major functionality operates within its own dedicated module: data preparation, model training, evaluation, and linguistic feature definitions. Interfaces between modules are well-defined, with explicit imports and type annotations ensuring proper integration. Configuration management is handled separately in a \verb|config| directories, allowing training parameters to be modified without changing implementation code. The evaluation pipeline is particularly well-separated, with distinct components handling LLM-as-a-judge evaluation, statistical significance testing, and visualization. This separation enables components to be developed, tested, and maintained independently, while facilitating reuse across different parts of the project. For example, the data validation utilities in \verb|src/data_prep/data_validators.py| are leveraged by both data preparation and model training components.

\numpara{CS162-communication} Documentation permeates every level of the project, from high-level design to implementation details. Module-level docstrings clearly articulate the purpose and functionality of each component. Function signatures include comprehensive docstrings with parameter descriptions, return values, and raised exceptions following Google style conventions. Type annotations throughout the codebase enhance clarity and enable static analysis tools to catch potential errors. The scripts directory includes well-documented entry points like \verb|scripts/train.p| that serve as executable examples of system usage. The README provides clear installation instructions, project overview, and usage examples for potential users and contributors. Pre-commit hooks ensure that documentation standards are maintained across contributions, while structured logging via the \verb|structlog| library creates informative runtime logs for debugging and monitoring. This comprehensive documentation approach ensures that the project remains accessible and maintainable despite its complexity.

% TODO: Capstone LOs
\subsection{Capstone LOs} \label{sec:capstone-los}

\numpara{navigation} The project development journey documented in \Cref{app:previous-iterations} reveals a systematic and adaptive approach to navigating complex technical challenges. Initially starting with Gemma-2-9b fine-tuning, the project pivoted to GRPO with Gemma-3-4b after evaluating the limitations of the initial approach. This decision was informed by a thorough analysis of the model's performance and resource constraints, leading to a more effective training strategy. The iterative process involved rounds of qualitative evaluations, including testing different model architectures and training methodologies, converging on a solution that balanced performance and resource efficiency. The project timeline reflects this adaptive navigation, with clear milestones and decision points documented in \Cref{app:previous-iterations}. Each transition was guided by empirical evidence and theoretical considerations, demonstrating a thoughtful approach to problem-solving in a new domain.

\numpara{outcomeanalysis} The project implements a multi-faceted evaluation approach to critically analyze the performance of the \linksys. The evaluation methodology in \Cref{sec:evaluation} combines quantitative metrics from LLM-as-a-judge evaluation with qualitative insights from pairwise preference, providing both breadth and depth of assessment. Statistical significance testing through Wilcoxon signed-rank tests for Likert scales and McNemar's test for boolean metrics ensures that reported improvements are robust rather than artifacts of random variation. Effect size calculations quantify the practical significance of improvements beyond mere statistical significance, with medium effect sizes for memorability (Cohen's d = 0.566) and clarity (Cohen's d = 0.444) validating the system's effectiveness in key areas. The evaluation framework was carefully designed to measure aspects informed by psycholinguistics. The analysis acknowledges limitations (\Cref{sec:limitations}), particularly regarding the use of LLM-as-a-judge methodology and limited human annotation sample sizes.

\numpara{curation} The curation process for the \links dataset reflects meticulous attention to source selection, quality control, and systematic organization. The vocabulary selection approach described in Section 4.1 combines multiple academic sources (TOEFL, IELTS, SAT) to ensure relevance and applicability to the project's educational aims. Deduplication through fuzzy matching with a 95\% threshold prevents redundancy while preserving meaningful semantic distinctions. The synthetic data generation process incorporates prompt design based on empirical testing of multiple approaches, selecting the 10-shot CoT method based on quantitative performance comparison. Quality control measures include automated format validation as well as manual review of a representative sample to verify linguistic grounding and association strength. The generated dataset is structured as triplets of reasoning trace, mnemonic, and example sentence, providing rich contextual information beyond simple vocabulary-definition pairs. This comprehensive curation approach resulted in a high-quality, coherent dataset that effectively supports both model training and evaluation, while establishing a standard for similar linguistic datasets in educational applications.

\numpara{qualitydeliverables} The project delivers multiple high-quality artifacts that demonstrate appropriate scope, depth, and rigor for a natural language processing application. The \linksys itself represents a complete, working implementation of linguistically grounded mnemonic generation, packaged for practical deployment in educational settings through local serving options like OpenWebUI and Ollama. The formal paper follows ACL formatting conventions with comprehensive sections covering theoretical foundations, methodology, empirical results, and limitations. The supporting codebase adheres to professional standards with comprehensive typing, modular organization, and thorough documentation. The evaluation methodology combines automated metrics with preference studies, providing robust evidence for system performance while acknowledging limitations. The taxonomy of linguistic features and VAM model of effective mnemonics contribute theoretical frameworks that extend beyond this specific implementation, providing generalizable knowledge for the field. Collectively, these deliverables demonstrate sophisticated scope in addressing a complex interdisciplinary challenge, appropriate depth in both technical implementation and theoretical foundation, and scholarly rigor in evaluation and documentation.

\subsection{HCs} \label{sec:hcs}
\numpara{audience} The project addresses dual audiences: (1) NLP researchers investigating LLMs' linguistic capabilities, for whom we provide detailed technical methodologies and evaluation metrics; and (2) educational technology developers seeking practical approaches to vocabulary learning assistance, for whom we demonstrate application potential and implementation strategies. For example, \Cref{sec:mnemonic-review} introduces key concepts about mnemonic devices and language teaching with sufficient depth for both audiences, while hiding technical details in appendices but providing pointers to those details in each section for interested readers. The paper's structure and content are designed to facilitate knowledge transfer across these domains, ensuring that both audiences can derive value from the research findings. Technical content is balanced with practical implications for vocabulary acquisition and language learning, ensuring relevance to both research and application-oriented readers.

\numpara{organization} The paper follows standard ACL formatting conventions, with a logical progression from theoretical foundations through methodology to empirical results. The structure facilitates efficient information extraction, with each section building upon previous content. Key contributions are identified early (\Cref{sec:intro}) and systematically developed throughout subsequent sections. Technical details that might interrupt argumentative flow are relegated to appendices, maintaining narrative coherence while ensuring methodological transparency for reproduction purposes. This organization aligns with expectations of the computational linguistics community while supporting efficient knowledge transfer.

\numpara{gapanalysis} \Cref{sec:intro} identifies critical limitations in existing mnemonic generation approaches: (1) overreliance on the keyword method, which fails for abstract vocabulary lacking concrete referents, and (2) neglect of the rich linguistic knowledge embedded in LLMs that could enable more diverse mnemonic strategies beyond simple keyword associations. Prior work has also passively delivered mnemonics to learners rather than leveraging individual learning preferences, despite research showing self-created mnemonics enhance retention. These identified gaps motivate our project to elicit linguistic reasoning and creativity from LLMs, enabling them to generate mnemonics that are not only effective but also linguistically grounded. Our final model, \linksys, after deployment, could interact with learners to tailor mnemonic generation based on their preferences, enhancing the learning experience. This approach addresses the limitations of existing methods by providing a more comprehensive and personalized vocabulary learning tool. The project also contributes to the field of educational technology by exploring how LLMs can be harnessed for creating effective resources for language education and self-study, such as creating mnemonic devices.

\numpara{hypothesisdevelopment} The research questions in \Cref{sec:intro} establish testable hypotheses regarding LLMs' capabilities for linguistic mnemonic generation. We hypothesize that \numlist{1} LLMs with reasoning capabilities can leverage various linguistic features beyond keyword associations to generate effective mnemonics without further training, \numlist{2} these capabilities can be distilled to smaller, locally deployable models, and \numlist{3} LLM-generated mnemonics can be comparably helpful to human-generated ones for memorability. These hypotheses are grounded in the theoretical understanding that LLMs encode substantial linguistic knowledge during pre-training that can be elicited through appropriate prompting and fine-tuning techniques. Each hypothesis is operationalized through specific evaluation metrics in \Cref{sec:evaluation}, with both quantitative measures and preference studies to ensure empirical validation. This structured approach to hypothesis formulation and testing enables systematic investigation of the potential for AI-assisted vocabulary learning.

\numpara{scienceoflearning} Our approach is deeply rooted in evidence-based principles from the science of learning, particularly for vocabulary acquisition. The VAM model (\Cref{fig:good-bad-mnemonics}) incorporates key learning principles including dual coding theory, generation effect, and appropriate levels of desirable difficulty. By emphasizing mnemonics that link new vocabulary to existing knowledge through meaningful linguistic connections, the system leverages the learning principle of building on prior associations. The emphasis on morphological breakdown in mnemonics facilitates chunking, helping learners organize information into manageable units. Our evaluation criteria specifically measure memorability factors identified in cognitive psychology research, such as concreteness, imageability, and distinctiveness (\Cref{sec:qualitative-llm-judge}). The system's integration of etymological information creates deeper processing opportunities, which research indicates enhances retention compared to surface-level associations. This evidence-based foundation ensures that the technological innovation serves genuine pedagogical needs rather than merely demonstrating technical capabilities.

\numpara{optimization} The GRPO implementation detailed in \Cref{app:grpo} represents a sophisticated optimization approach to distilling linguistic reasoning capabilities. The objective function combines multiple reward components weighted according to their relative importance for mnemonic quality, with higher values assigned to linguistic grounding (criterion 3). The decision variables include model parameters controlled through low-rank adaptation matrices, significantly reducing the dimensionality of the optimization space compared to full-parameter tuning. Constraints are implemented both explicitly through penalties on undesirable patterns (like acronyms) and implicitly through reference policy anchoring via KL divergence terms. We carefully balanced exploration vs. exploitation by generating multiple candidates ($G=2$) per training example, enabling the model to learn from comparisons rather than absolute reward values. The clipping mechanism in the objective prevents excessive policy updates that might destabilize training. This optimization approach successfully navigates the complex trade-offs between computational efficiency, linguistic quality, and generalization capability while working within the severe memory constraints of consumer hardware.

\numpara{algorithms} The training methodology incorporates multiple algorithmic innovations to address the challenges of distilling reasoning capabilities to smaller models. The core Group Relative Policy Optimization (GRPO) algorithm detailed in \Cref{app:grpo} extends standard reinforcement learning approaches by generating multiple completions for each prompt and normalizing rewards within these groups, enabling more stable policy updates compared to absolute reward signals. We enhance this with rank-stabilized LoRA (rsLoRA) adaptation, modifying the standard scaling factor to improve stability across different ranks (\Cref{app:finetuning}). The reward computation combines multiple linguistic quality signals through weighted aggregation, effectively transforming the subjective notion of "good mnemonics" into a quantifiable optimization target. For inference, we implement a structured decoding process based on recommendations from the Gemma-3 team, balancing creativity and coherence in the generated mnemonics. These algorithmic choices collectively enable efficient training on limited hardware while preserving the sophisticated reasoning capabilities required for effective mnemonic generation.

\numpara{heuristics} Our approach employs several problem-solving heuristics to enhance mnemonic generation. For example, we used psycholinguistic insights, an adjacent field of linguistics and memory, to create VAM model and inform our reward functions. We use problem decomposition by separating mnemonic creation into linguistic analysis and creative association phases, enabling more systematic knowledge utilization. We also used diagram \Cref{fig:distillation} to illustrate the distillation process, breaking down complex interactions into manageable components. This heuristic approach facilitates understanding and implementation of the system.

\numpara{sampling} The evaluation methodology employs stratified random sampling to ensure representation across linguistic feature categories (\Cref{sec:vocab-selection}) and vocabulary complexity levels. For human evaluation, we randomly selected 50 test examples, stratified by linguistic feature, to obtain less biased assessments of mnemonic quality. This sampling strategy ensures balanced representation of different mnemonic types while maintaining statistical validity. Each example received multiple independent ratings to mitigate individual rater bias, with inter-rater reliability calculated using Cohen's Kappa score to confirm consistency in ratings.

\numpara{dataviz} We employ targeted data visualizations to communicate complex relationships between linguistic features and mnemonic effectiveness. Radar charts display the distribution of linguistic features across different model outputs, while heatmaps visualize correlation patterns between computational metrics and human evaluations. Embedding space projections illustrate semantic relationships between vocabulary terms and their mnemonics, providing intuitive visual confirmation of semantic relevance scores. These visualizations enhance interpretability of results while supporting our conclusions regarding the contribution of different linguistic features to mnemonic quality.

\numpara{significance} Statistical significance testing (\Cref{sec:qualitative-llm-judge}) confirms the reliability of our comparative results between baseline and \linksys. We employ paired statistical tests (Wilcoxon signed-rank and McNemar test) to account for vocabulary-specific variation when comparing model outputs on identical test sets. Effect size calculations quantify the practical significance of improvements, while confidence intervals provide transparency regarding the precision of our estimates. Multiple comparison corrections maintain statistical rigor when evaluating performance across different linguistic feature categories.

\numpara{shapingbehavior} The intended application of our approach shapes vocabulary learning behavior by encouraging deeper engagement with linguistic features. Rather than keyword method, the generated mnemonics prompt learners to recognize morphological, etymological, and semantic patterns, fostering more robust mental representations. By explicitly highlighting these linguistic features, the system promotes analytical processing of vocabulary, which research indicates enhances long-term retention. This behavior-shaping aspect represents a significant advantage over keyword-only approaches that rely on shallow phonetic or orthographic associations.

\numpara{biasmitigation} Our project implements several strategies to mitigate potential biases in mnemonic generation. First, we address linguistic diversity concerns through our taxonomy of linguistic features (\Cref{tab:linguistic-features}), which encourages the model to explore multiple linguistic dimensions rather than defaulting to dominant patterns or Western-centric etymologies. During in-context learning experiments (\Cref{sec:icl-performance}), we discovered terminology bias where models associated "mnemonic" primarily with acronyms, addressing this by testing alternative phrasings like "memory cue" which increased linguistically grounded responses. The GRPO reward functions (\Cref{sec:training-inference}) explicitly penalize low-effort approaches like acronyms, while preferentially rewarding deeper linguistic analysis. For vocabulary selection (\Cref{sec:vocab-selection}), we combined sources from different standardized tests (TOEFL, IELTS, GRE, SAT) and CEFR levels to ensure diversity of origins, complexity, and domains. We mitigated positional and verbosity biases in our LLM-as-a-judge evaluation by employing structured protocols with shuffled presentation order and controlled output formats (\Cref{sec:qualitative-llm-judge}). Our evaluation metrics were designed to assess multiple dimensions of mnemonic quality rather than privileging Western educational norms, acknowledging in limitations (\Cref{sec:limitations}) that additional work is needed to expand beyond English to serve diverse linguistic communities. These bias mitigation strategies collectively aim to produce a system that supports vocabulary learning across different cultural an

\numpara{ethicalconsiderations} Our work addresses ethical considerations in educational technology deployment. We explore linguistic diversity by analyzing and leveraging several linguistic features for mnemonic generation, ensuring that the generated mnemonics are inclusive and accessible to learners from diverse linguistic backgrounds. The model is trained on a diverse dataset of vocabulary words, including those from various languages and etymological origins, to ensure that the mnemonics generated are relevant and culturally sensitive. We also consider the potential for bias in the generated mnemonics by ensuring that the training data is representative of a wide range of vocabulary words.

\numpara{studyreplication} To facilitate replication, we provide comprehensive implementation details in \Cref{app:training-details}, including environment setup, model parameters, and training configurations. The dataset construction process is documented in \Cref{sec:data-gen}, with preprocessing steps explicitly specified. All code and datasets are made publicly available through GitHub and HuggingFace repositories, with standardized formats ensuring compatibility with common ML frameworks. This transparency ensures that other researchers can validate the findings and build upon the methodology.
