# Mnemonic Generation for English Words

[![Model on HF](https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md-dark.svg)](https://huggingface.co/chiffonng/gemma2-9b-it-mnemonics) [![Dataset on HF](https://huggingface.co/datasets/huggingface/badges/resolve/main/dataset-on-hf-md-dark.svg)](https://huggingface.co/datasets/chiffonng/en-vocab-mnemonics)

Vocabulary acquisition poses a significant challenge for language learners, particularly at medium and advanced levels, where the complexity and volume of new words can hinder retention. One promising solution is mnemonics, which leverage associations between new vocabulary and memorable cues to enhance recall. Previous efforts to automate generating these mnemonics often focus primarily on _shallow-encoding mnemonics_ (spelling or phonological features of a word) and have lower likelihood of improving retention, compared to _deep-encoding information_.

This project explores an alternative approach by instruction tuning the Gemma 2 (9B) language model on a manually curated dataset of over 1,000 examples. Unlike prior methods, this dataset includes more _deep-encoding mnemonics_ (semantic information such as morphology and etymology, associations with synonyms, antonyms, or related words and concepts). By fine-tuning the model on this diverse dataset, we aim to improve the quality and variety of mnemonics generated by the model, and improve the retention of new vocabulary for language learners.

| **Shallow-Encoding Mnemonics**                                                      | **Deep-Encoding Mnemonics**                                                                                  |
| ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Homophonic:** olfactory sounds like "old factory."                                | **Etymology**: preposterous - pre (before) + post (after) + erous, which implies absurd.                     |
| **Chunking:** obsequious sounds like "ob-se-ki-ass. Obedient servants kiss your ass | **Morphology:** Suffixes "ate" are usually verbs. Prefix "ab" means from, away.                              |
| **Keyword:** Phony sounds like “phone-y,” which means fraudulent (phone calls).     | **Context/Story:** His actions of pouring acid on the environment are detrimental                            |
| **Rhyming:** wistful/longing for the past but wishful for the future.               | **Synonym/Antonym** "benevolent" ~ "kind" or "generous," and "malevolent" is its antonym.                    |
|                                                                                     | **Image Association:** exuberant - The ex-Uber driver never ranted; he always seems ebullient and lively.    |
|                                                                                     | **Related words**: Phantasm relates to an illusion or ghostly figure, closely tied to the synonym “phantom.” |

---

## Project components

- [ ] A web interface (using Gradio) for the tuned model.
- [x] A dataset of 1200 examples of English words with mnemonics.
- [ ] This documented codebase.

## Setup

### Installation

> **DEVELOPMENT MODE**: The development alternates between MacOS, Google Colab and Google Cloud Deep Learning virtual machine (Linux, NVIDIA T4). The first two use `uv` and the last use `conda`, so the setup script and dependencies files are NOT stable.

Currently `conda` is the recommended way to install the dependencies:

```bash
conda env create -n mnemonic-gen python=3.10 -f environment.yaml
conda activate mnemonic-gen
```

Otherwise, you can try the setup script:

```bash
bash setup.sh
```

It attempts to install with [uv](https://docs.astral.sh/uv/) (a fast, Rust-based Python package and project manager) using `.python-version` file and `pyproject.toml` file. This is the recommended way to manage the project, since its resolver is faster and more reliable than `pip`.

Otherwise, it falls back to `pip` installation.

### Secrets

Create a `.env` by cloning `.env.template`. You will need:

- OpenAI API key (optional: for some modules inside `src/data_pipeline`)
- Hugging Face Access Token. You will need at least `read` access token to load the dataset and model from Hugging Face (see the [doc](https://huggingface.co/docs/hub/en/security-tokens)). You can get it from [here](https://huggingface.co/settings/token).
- Wandb API key (optional: for logging experiments). You can get it from [here](https://wandb.ai/authorize).

## Development

```bash
pre-commit install
pre-commit run --all-files
```
